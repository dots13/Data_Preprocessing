{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: Feature selection, transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import norm, ttest_ind\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression, RFECV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_log_error, make_scorer, mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Distribution of the target varaible\n",
    "# sns.distplot(df.SalePrice, fit=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Observe skewness and kurtosis as measures of asymetry and long tails\n",
    "# print(df.SalePrice.skew())\n",
    "# print(df.SalePrice.kurtosis())\n",
    "\n",
    "# # Check log transform\n",
    "# print(np.log1p(df.SalePrice).skew())\n",
    "# print(np.log1p(df.SalePrice).kurtosis())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to transform to obtain normal distribution\n",
    "# sns.distplot(np.log1p(df.SalePrice), fit=norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log transform of SalePrices\n",
    "# df.SalePrice = np.log1p(df.SalePrice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple clean & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove Miscellaneous feature value since we don't want to create several value variables for each feature (try it at home)\n",
    "# df.drop(['MiscVal'], axis=1, inplace = True, errors = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show features with largest number of missing values\n",
    "# display(df.isnull().sum().nlargest(5))\n",
    "# # find features with more than half missings\n",
    "# half_miss_columns = df.columns[df.isnull().sum() > len(df)/2]\n",
    "# # drop\n",
    "# df.drop(half_miss_columns, axis = 1, inplace = True)\n",
    "# # see result\n",
    "# display(df.isnull().sum().nlargest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Fireplaces are connected with fireplace quality - i.e data are cleaned\n",
    "# df[df.Fireplaces != 0].FireplaceQu.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill all NaN with 0\n",
    "# df.fillna(0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This point is not necessary for later get_dummies\n",
    "# See unique values for object type\n",
    "# display(df.select_dtypes(include=['object']).nunique().nlargest(5))\n",
    "# # Convert all object values to categorial format\n",
    "# df[df.select_dtypes(include=['object']).columns] = df.select_dtypes(include=['object']).apply(pd.Series.astype, dtype='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert all Area and Square feet to log values\n",
    "# for column in df.filter(regex='Area|SF', axis=1).columns:\n",
    "#     df['Has' + column] = (df[column] > 0).replace({True: 1, False: 0}).astype('uint8')\n",
    "# #     df.loc[df[column] > 0, column] = np.log(df.loc[df[column] > 0, column])\n",
    "#     df['Sqrt' + column] = np.sqrt(df[column])\n",
    "#     df['Log' + column] = np.log1p(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df[df.filter(regex='Area', axis=1).columns].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sns.distplot(df[df.LotArea>0].LogLotArea, fit = norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert categorical variables to indicators and create new data\n",
    "# # This is usefull for regression (resonable model) - in the case of classification this is usually not necessary \n",
    "# df = pd.get_dummies(df)\n",
    "# # see how many of each type we have\n",
    "# df.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove constant features\n",
    "# display(df.columns[df.min() == df.max()])\n",
    "# df = df[df.columns[df.min() != df.max()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# collect type column names\n",
    "# continuous_columns = df.select_dtypes(include=['float64']).columns\n",
    "# discrete_columns = df.select_dtypes(include=['int64']).columns\n",
    "# indicator_columns = df.select_dtypes(include=['uint8']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train & test data to demonstrate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dt, dtest = train_test_split(df, test_size=0.25, random_state=17)\n",
    "# dt = dt.copy()\n",
    "# dtest = dtest.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print('Train: ', len(dt), '; Test: ', len(dtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check max and min values for indicators  - should be stored in uint8\n",
    "# print(dt.select_dtypes(include=['uint8']).min().min())\n",
    "# print(dt.select_dtypes(include=['uint8']).max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Standardize scaling\n",
    "# scaler = preprocessing.StandardScaler()\n",
    "\n",
    "# # Prepare column names - ['float64', 'int64', 'uint8']\n",
    "# columns = dt.select_dtypes(include=['float64', 'int64']).columns\n",
    "# columns = columns.drop('SalePrice', errors = 'ignore')\n",
    "# print(list(columns))\n",
    "\n",
    "# # Prepare values for transform\n",
    "# scaler.fit(dt[columns])\n",
    "\n",
    "# # Transform\n",
    "# dt[columns] = scaler.transform(dt[columns])\n",
    "# dtest[columns] = scaler.transform(dtest[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dt.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sns.boxplot(data=dt[['OverallQual']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find irelevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Drop Id\n",
    "# dt.drop(['Id'], axis = 1, errors = 'ignore', inplace = True)\n",
    "# dtest.drop(['Id'], axis = 1, errors = 'ignore', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Find features with low variance\n",
    "# columns_to_remove = dt.columns[dt.var() < 0.02]\n",
    "# dt[columns_to_remove].var().nlargest(5)\n",
    "# print(len(columns_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Perform t-test with indicator variables - calculate p-values\n",
    "# ttest_pvals = df\\\n",
    "#     .drop(columns_to_remove, axis = 1, errors = 'ignore')\\\n",
    "#     .select_dtypes(include = ['uint8']).columns\\\n",
    "#     .to_series()\\\n",
    "#     .apply(lambda x: ttest_ind(df.SalePrice[df[x] == 0], df.SalePrice[df[x] == 1], equal_var = False).pvalue)\n",
    "\n",
    "# # show largest and smallest\n",
    "# display(ttest_pvals.nlargest(5))\n",
    "# display(ttest_pvals.nsmallest(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(ncols=2, figsize=(15,7))\n",
    "# sns.boxplot(x='Exterior1st_Plywood', y='SalePrice', data=dt,ax=axs[0])\n",
    "# sns.boxplot(x='ExterQual_TA', y='SalePrice', data=dt,ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Remove all largest than 50%\n",
    "# columns_to_remove = list(set(columns_to_remove).union(set(ttest_pvals[ttest_pvals > 0.5].index)))\n",
    "# print(len(columns_to_remove))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Correlation matrix Pearson Spearman\n",
    "# corrP = dt.drop(columns_to_remove, axis = 1, errors = 'ignore').corr(method='pearson')\n",
    "# corrS = dt.drop(columns_to_remove, axis = 1, errors = 'ignore').corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Pearson top 10\n",
    "# corrP_cols = corrP.SalePrice.abs().nlargest(10).index\n",
    "# display(corrP.SalePrice.loc[corrP_cols])\n",
    "# # Spearman top 10\n",
    "# corrS_cols = corrS.SalePrice.abs().nlargest(10).index\n",
    "# display(corrS.SalePrice.loc[corrS_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Plot those correlations\n",
    "# fig, axs = plt.subplots(ncols=2, figsize=(15,7))\n",
    "# sns.heatmap(corrP.abs().loc[corrP_cols,corrP_cols],ax=axs[0], cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=corrP_cols.values, xticklabels=corrP_cols.values)\n",
    "# sns.heatmap(corrS.abs().loc[corrS_cols,corrS_cols],ax=axs[1], cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=corrS_cols.values, xticklabels=corrS_cols.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # OverallQual is actually orderd categorical variable stored as number\n",
    "# sns.boxplot(x='OverallQual', y='SalePrice', data=dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Use scikit learn to select best features\n",
    "# # prepare data\n",
    "# X = dt.drop(columns_to_remove, axis = 1, errors = 'ignore').drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "# y = dt.SalePrice\n",
    "\n",
    "# # calculate F values for correlation coefficient (calculated from rho using an increasing function)\n",
    "# # calculate p-values of the correspondong F-test (inversely proportional to rho)\n",
    "# Fscores, pvals = f_regression(X,y)\n",
    "# print(max(pvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display(pd.Series(pvals, index=X.columns).nsmallest(5))\n",
    "# display(corrP.SalePrice.abs().nlargest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # We can also use Mutual Information (information gain) - but for linear regression - correlation is usually better\n",
    "# mi = mutual_info_regression(X,y)\n",
    "# display(pd.Series(mi, index=X.columns).nlargest(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Stupid selection of n performing highest correlations\n",
    "# n = 100\n",
    "# columns_to_leave = corrP.SalePrice.abs().nlargest(n).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Select by linear regression\n",
    "# X = dt.drop(columns_to_remove, axis = 1, errors = 'ignore').drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "# # X = dt.drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "# y = dt.SalePrice\n",
    "\n",
    "# used_columns = X.columns\n",
    "\n",
    "# def scorer(Y, yp):\n",
    "#     return np.sqrt(mean_squared_error(Y, yp))\n",
    "\n",
    "# clfM = LinearRegression()\n",
    "\n",
    "# selector = RFECV(clfM, step=1, cv=5, scoring=make_scorer(scorer))\n",
    "# selector = selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # transform result to dataframe\n",
    "# result = pd.DataFrame({'Chosen': selector.support_, 'Ranking': selector.ranking_}, index=list(used_columns))\n",
    "# # columns to leave\n",
    "# columns_to_leave = result[result.Chosen == True].index\n",
    "# # show results\n",
    "# display(result[result.Chosen == False].head(5))\n",
    "# display(result.Chosen.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Data prepare\n",
    "# X = dt[columns_to_leave].drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "# y = dt.SalePrice\n",
    "# Xtest = dtest[columns_to_leave].drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "# ytest = dtest.SalePrice\n",
    "\n",
    "# # Linear Regression\n",
    "# clf1 = LinearRegression()\n",
    "# clf1.fit(X, y) \n",
    "\n",
    "# # Print RMSLE\n",
    "# print('Root mean squared logarithmic error:', np.sqrt(mean_squared_error(clf1.predict(Xtest), ytest)))\n",
    "\n",
    "# # Joint Plot\n",
    "# sns.jointplot(ytest, clf1.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation  - Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ytest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-a8a1097107d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# # Joint Plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjointplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mytest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ytest' is not defined"
     ]
    }
   ],
   "source": [
    "# # Data prepare with all features\n",
    "# X = dt.drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "# # X = X.drop(columns_to_remove, axis = 1, errors = 'ignore')\n",
    "# y = dt.SalePrice\n",
    "\n",
    "# Xtest = dtest.drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "# # Xtest = Xtest.drop(columns_to_remove, axis = 1, errors = 'ignore')\n",
    "# ytest = dtest.SalePrice\n",
    "\n",
    "# # Determine alpha using cross validation\n",
    "# def scorer(Y, yp):\n",
    "#     return np.sqrt(mean_squared_error(Y, yp))\n",
    "\n",
    "# def fun(alpha):\n",
    "#     clf = Ridge(alpha=alpha)\n",
    "#     return np.mean(cross_val_score(clf, X, y, cv=5, scoring=make_scorer(scorer)))\n",
    "\n",
    "# # minimize\n",
    "# opt_alpha = minimize_scalar(fun, options = {'maxiter': 30}, method = 'bounded', bounds=(0.1, 400))\n",
    "# print(opt_alpha)\n",
    "\n",
    "# # Ridge regression\n",
    "# clf2 = Ridge(alpha=opt_alpha.x)\n",
    "# clf2.fit(X, y) \n",
    "\n",
    "# # Print RMSLE\n",
    "# print('Root mean squared logarithmic error:', np.sqrt(mean_squared_error(clf2.predict(Xtest), ytest)))\n",
    "\n",
    "# # Joint Plot\n",
    "sns.jointplot(ytest, clf2.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation - Kernel Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Data prepare with all features\n",
    "# X = dt.drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "# # X = X.drop(columns_to_remove, axis = 1, errors = 'ignore')\n",
    "# y = dt.SalePrice\n",
    "\n",
    "# Xtest = dtest.drop(['SalePrice'], axis = 1, errors = 'ignore')\n",
    "# # Xtest = Xtest.drop(columns_to_remove, axis = 1, errors = 'ignore')\n",
    "# ytest = dtest.SalePrice\n",
    "\n",
    "# # Determine alpha using cross validation\n",
    "# def scorer(Y, yp):\n",
    "#     return np.sqrt(mean_squared_error(Y, yp))\n",
    "\n",
    "# def fun(alpha):\n",
    "#     clf = KernelRidge(alpha=alpha)\n",
    "#     return np.mean(cross_val_score(clf, X, y, cv=5, scoring=make_scorer(scorer)))\n",
    "\n",
    "# # minimize\n",
    "# opt_alpha = minimize_scalar(fun, options = {'maxiter': 40}, method='bounded', bounds=(0.001, 100))\n",
    "# print(opt_alpha)\n",
    "\n",
    "# # Ridge regression\n",
    "# clf3 = KernelRidge(alpha=opt_alpha.x)\n",
    "# clf3.fit(X, y) \n",
    "\n",
    "# # Print RMSLE\n",
    "# print('Root mean squared logarithmic error:', np.sqrt(mean_squared_error(clf3.predict(Xtest), ytest)))\n",
    "\n",
    "# # Joint Plot\n",
    "# sns.jointplot(ytest, clf3.predict(Xtest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework task 3:\n",
    "* Load homework_data.csv.\n",
    "* Analyze the distribution of 'price' and try to transform it into normal distibution.\n",
    "* Perform proper feature transformations.\n",
    "* Remove all irelevant features.\n",
    "* Select a proper subset of features.\n",
    "* Try to construct linear regression model to predict prices. Evaluate its root mean squared logarithmic error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dh = pd.read_csv('homework_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
